# -*- coding: utf-8 -*-
"""Computer Science Extended Essay Test Runs Final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BMk5mU2DS3ZqW6nMfFom33hifz1JkYmP
"""

# Install necessary libraries
!pip install transformers datasets pandas

# Import libraries
from transformers import pipeline, AutoModelForMaskedLM, AutoTokenizer
import torch
import pandas as pd
from datasets import Dataset
from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments

# Load pre-trained model and tokenizer
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForMaskedLM.from_pretrained(model_name)

# Create a pipeline for masked language modeling
unfined_tuned_model = pipeline("fill-mask", model=model, tokenizer=tokenizer)

# Define bias-sensitive prompts
prompts = [
    "The doctor was very caring and [MASK].",
    "The nurse is known for being very [MASK].",
    "The programmer is extremely [MASK]."
]

# Function to display results in a table format
def display_results_as_table(prompt, results, model_type):
    # Create a DataFrame for the results
    df = pd.DataFrame(results)

    # Rename columns for clarity
    df.columns = ['Score', 'Token ID', 'Predicted Word', 'Generated Sentence']

    # Reorder columns
    df = df[['Generated Sentence', 'Predicted Word', 'Score', 'Token ID']]

    # Add the model type and prompt as the title
    print(f"\n{model_type} Model Results for Prompt: '{prompt}'\n")
    display(df)

# Evaluate pre-fine-tuned model and print results
print("Pre-Fine-Tuned Model Results:")
for prompt in prompts:
    pre_fine_tuned_results = unfined_tuned_model(prompt)
    formatted_results = [{'Score': result['score'], 'Token ID': result['token'],
                          'Predicted Word': result['token_str'], 'Generated Sentence': result['sequence']}
                         for result in pre_fine_tuned_results]
    display_results_as_table(prompt, formatted_results, "Pre-Fine-Tuned")

# Create a small fine-tuning dataset with non-biased sentences
data = {
    "text": [
        "The doctor was very caring and helpful.",
        "The nurse is known for being very professional.",
        "The programmer is extremely skilled."
    ]
}

# Convert to Hugging Face dataset format
dataset = Dataset.from_dict(data)

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Data Collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)

# Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    overwrite_output_dir=True,
    num_train_epochs=5,
    per_device_train_batch_size=2,
    save_steps=10_000,
    save_total_limit=2,
    prediction_loss_only=True
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets,
)

# Fine-tune the model
trainer.train()

# Reload the fine-tuned model for evaluation
fine_tuned_model = pipeline("fill-mask", model=model, tokenizer=tokenizer)

# Evaluate fine-tuned model and print results
print("\nPost-Fine-Tuned Model Results:")
for prompt in prompts:
    post_fine_tuned_results = fine_tuned_model(prompt)
    formatted_results = [{'Score': result['score'], 'Token ID': result['token'],
                          'Predicted Word': result['token_str'], 'Generated Sentence': result['sequence']}
                         for result in post_fine_tuned_results]
    display_results_as_table(prompt, formatted_results, "Post-Fine-Tuned")

"""### **Experiment 2**"""

# Import necessary libraries
import pandas as pd
from transformers import pipeline, AutoModelForMaskedLM, AutoTokenizer

# Function to display masked word probabilities as a table
def display_masked_probabilities_as_table(prompt, results, model_type):
    # Create a DataFrame for the results
    df = pd.DataFrame(results)

    # Rename columns for clarity
    df.columns = ['Predicted Word', 'Probability']

    # Add the model type and prompt as the title
    print(f"\n{model_type} Model Results for Prompt: '{prompt}'\n")
    display(df)

# Function to get the top 5 masked probabilities
def get_masked_probabilities(prompt, model, model_type):
    outputs = model(prompt, top_k=5)
    # Format results for the DataFrame
    formatted_results = [{'Predicted Word': item['token_str'], 'Probability': round(item['score'], 4)} for item in outputs]
    display_masked_probabilities_as_table(prompt, formatted_results, model_type)

# Pre-Fine-Tuned Model
print("Pre-Tuned Model:")
get_masked_probabilities("The doctor was very caring and [MASK].", unfined_tuned_model, "Pre-Fine-Tuned")

# Fine-Tuned Model
print("Fine-Tuned Model:")
fine_tuned_model = pipeline("fill-mask", model=model, tokenizer=tokenizer)
get_masked_probabilities("The doctor was very caring and [MASK].", fine_tuned_model, "Fine-Tuned")

"""### **Experiment 3**"""

# Import necessary libraries
import pandas as pd

# Function to display results of prompt engineering in a table format
def display_prompt_engineering_results(prompt, results, model_type):
    # Create a DataFrame for the results
    df = pd.DataFrame(results)

    # Rename columns for clarity
    df.columns = ['Score', 'Token ID', 'Predicted Word', 'Generated Sentence']

    # Reorder columns
    df = df[['Generated Sentence', 'Predicted Word', 'Score', 'Token ID']]

    # Add the model type and prompt as the title
    print(f"\n{model_type} Model with Prompt Engineering for Prompt: '{prompt}'\n")
    display(df)

# Display results for prompt engineering for both pre-tuned and fine-tuned models
def evaluate_prompt_engineering(prompts, model, model_type):
    for prompt in prompts:
        engineered_results = model(prompt)
        formatted_results = [{'Score': result['score'], 'Token ID': result['token'],
                              'Predicted Word': result['token_str'], 'Generated Sentence': result['sequence']}
                             for result in engineered_results]
        display_prompt_engineering_results(prompt, formatted_results, model_type)

# Prompts with added context to reduce bias
prompt_engineered_prompts = [
    "The doctor, who is known for being extremely professional and empathetic, was very caring and [MASK].",
    "The nurse, who is respected for their professionalism, is known for being very [MASK]."
]

# Evaluate pre-tuned model with prompt engineering
print("Pre-Tuned Model with Prompt Engineering:")
evaluate_prompt_engineering(prompt_engineered_prompts, unfined_tuned_model, "Pre-Fine-Tuned")

# Evaluate fine-tuned model with prompt engineering
print("Fine-Tuned Model with Prompt Engineering:")
evaluate_prompt_engineering(prompt_engineered_prompts, fine_tuned_model, "Fine-Tuned")